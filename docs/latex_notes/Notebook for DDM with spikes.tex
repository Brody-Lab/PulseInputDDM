\documentclass[12pt]{article}

\usepackage{amsmath,empheq}
\usepackage{palatino}
\usepackage{txfonts}
\usepackage{bm}
\usepackage{graphicx}
\usepackage{color}
\usepackage{cases}
\usepackage[]{mcode}

\newcommand{\be}{\begin{equation}}
\newcommand{\ee}{\end{equation}}

\usepackage{natbib} %bibliography
\usepackage[font=footnotesize]{caption}

\usepackage[colorlinks=true, linkcolor=black, citecolor=black, urlcolor=blue]{hyperref}
\frenchspacing

\setlength\parindent{0pt}

\begin{document}

\section*{ML on joint behavioral-neural latent variable model}

Assuming independence of trials the likelihood of the data under the model is

\be
P(Y, D \mid \Theta, X) = \prod_{i} P(y^i, d^i \mid \Theta, x^i)
\ee

where $y^i$ is the spike count data on trial $i$, $d^i$ is the decision data on trial $i$, $x^i$ is all of the input stimuli on trial $i$ and $\Theta$ are all of model parameters. I will drop the $i$ henceforth, since I will only compute the likelihood for single trial.\\

We introduce a latent variable for trial $i$, $a$, and seek to compute

\be
P(y, d \mid  \Theta, x) = \int_{a} P(y, d, a \mid  \Theta, x) da,
\ee

so that we can perform inference on $\Theta$. Henceforth, the dependence on $\Theta$ and $x$ will be dropped, since they are constant over the computation of $P(y,d)$.

\begin{figure}[h!]

	\centerline {\includegraphics[width=4in]{DDM-1.pdf}}
	\caption{Graphical model depicting the relationship between various parameters, variables and data.}
	
	\label{fig:fig1}
	
\end{figure}

The latent process evolves for $T$ time steps and it is Markov, as shown in \hyperref[fig:fig1]{Figure \ref*{fig:fig1}}, which can be exploited to compute $P(y,d)$. The following recursion can be written:

\be
P(y_{1:t},a_{t}) = P(y_t \mid a_t) P(y_{1:t-1}, a_{t-1}) P(a_t \mid a_{t-1}).
\ee

From this, we can write:

\be
P(y_{1:T},d, a_{T}) = P(d \mid a_T) \prod_{t=2}^T P(y_t \mid a_t) P(a_t \mid a_{t-1}) P(y_1 \mid a_1) P(a_1).
\ee

and finally:

\be
\label{eq:posteriordata}
P(y_{1:T},d) = \int_{a_T} P(d \mid a_T) \prod_{t=2}^T P(y_t \mid a_t) P(a_t \mid a_{t-1}) P(y_1 \mid a_1) P(a_1) da_T.
\ee

From this, it appears that the likelihood can be evaluated by only computing the marginal over the final latent state. This is surprising to me, since I assumed that I would need to compute:

\be
P(y,d) = \int_a P(y,d,a).
\ee

%The first term in \hyperref[eq:posteriordata]{Equation \ref*{eq:posteriordata} } is as in the previous model:
%
%\be
%P(d = D \mid  a_T) = \int_{a_T=-\infty}^{bias} da_T P(a_T).
%\ee

Each term in \hyperref[eq:posteriordata]{Equation \ref*{eq:posteriordata}} is familiar or straightforward. The first term is the likelihood of the decision, given the final latent state. $y_t$ are the binned spikes from $N$ neurons where all neuron are independent conditioned on $a_t$, so the second term is the likelihood of some spiking activity at the current time-step, given the latent process at that time. This can be factorized across neurons:

\be
P(y_t \mid a_t) = \prod_{n} P(y_{t,n} \mid a_t),
\ee

%\be
%P(a_t \mid y_{t}) = \frac{P(y_{t} \mid a_t)P(a_t)}{\int P(y_{t} \mid a_t)P(a_t) da_t}
%\ee

where the likelihood for each neuron is:

\be
P(y_{n,t} \mid a_t) \propto {\lambda_{n,t}}^{y_{n,t}} e^{-\lambda_{n,t}}
\ee

\be
\lambda_{n,t} = a_n + \frac{b_n}{1 + e^{(-c_n \cdot a_t + d_n)}}
\ee

\end{document}
  